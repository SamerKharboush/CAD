{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "class UNetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_dir,reshape=True,height=128,width=128,autoencoder=False):\n",
    "        self.dataDirectory = data_dir\n",
    "        self.CAD_class = glob(data_dir+'/yes/*')\n",
    "        #self.yes_class = glob(data_dir+'/yes/*')\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.reshape=reshape\n",
    "        self.autoencoder = autoencoder\n",
    "        \n",
    "        labels = [0 for i in range(len(self.CAD_class))]\n",
    "        #labels += [1 for i in range(len(self.yes_class))]\n",
    "        \n",
    "        image_links = self.CAD_class #+ self.yes_class\n",
    "        self.dataframe = pd.DataFrame({\n",
    "            'image':image_links,\n",
    "            'labels': labels\n",
    "        })\n",
    "        \n",
    "        self.dataframe = shuffle(self.dataframe)\n",
    "        self.dataframe.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.CAD_class)#+len(self.yes_class)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        image_list = self.dataframe['image'][idx]\n",
    "        label_list = self.dataframe['labels'][idx]\n",
    "        \n",
    "        if type(image_list) == str:\n",
    "            image_list = [image_list]\n",
    "\n",
    "        if not isinstance(label_list,np.int64):\n",
    "            label_list = label_list.values\n",
    "\n",
    "        image_array = []\n",
    "        for image in image_list:\n",
    "            image = Image.open(image).convert(\"L\")\n",
    "            if self.reshape:\n",
    "                image = image.resize((self.height,self.width))\n",
    "\n",
    "            array = np.asarray(image)\n",
    "        \n",
    "            if self.autoencoder:\n",
    "                array = array.reshape(self.height*self.width)\n",
    "            else:\n",
    "                array = array.reshape(1,self.height,self.width)\n",
    "            \n",
    "            image_array.append(array)\n",
    "        \n",
    "        return [torch.tensor(image_array,device=device),torch.tensor(label_list,device=device)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  labels\n",
      "0  C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask...       0\n",
      "1  C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask...       0\n",
      "2  C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask...       0\n",
      "3  C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask...       0\n",
      "4  C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask...       0\n"
     ]
    }
   ],
   "source": [
    "dataset = UNetDataset(r\"C:\\Users\\Samer\\Downloads\\ECG-project\\CAD-Flask\\dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 2)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(1,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(1,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=2, retain_dim=False, out_sz=(128,128)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 2)\n",
    "        self.retain_dim  = retain_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): Encoder(\n",
       "    (enc_blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (2): Block(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (3): Block(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (4): Block(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (upconvs): ModuleList(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (dec_blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (2): Block(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "      (3): Block(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Conv2d(64, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10472/4117707447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 32\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for n in range(len(dataset)//batch_size):\n",
    "    \n",
    "        data,target = dataset[n*batch_size:(n+1)*batch_size]\n",
    "        #target = target.squeeze(0)\n",
    "     \n",
    "\n",
    "        ypred = model.forward(data.float())\n",
    "        loss = loss_fn(ypred,target)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "    loss_list.append(total_loss/batch_size)\n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epochs: {epoch} Loss: {total_loss/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "losses = [ loss.detach().numpy() for loss in loss_list]\n",
    "plt.plot(list(range(epochs)),losses)\n",
    "plt.title(\"Loss v/s Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0:'no',1:'yes'}\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(20):\n",
    "    data,target = dataset[i]\n",
    "    pred = model.forward(data.float())\n",
    "    pred = torch.argmax(pred,dim=1)\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.imshow(data[0][0].cpu())\n",
    "    plt.title(f'Actual: {mapping[target.cpu().detach().item()]} Predicted: {mapping[pred.cpu().detach().item()]}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5034687d4dec08762a8fb8c75beaa09e326dabf36d6301ba35ed15cf4f88efcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
